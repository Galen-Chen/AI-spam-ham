{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_dir = \"../input/\"\n",
    "df = pd.read_csv(data_dir + '/spam.csv', encoding='latin-1')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df.v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拆分过后的每个邮件内容\n",
      "1114    No no:)this is kallis home ground.amla home to...\n",
      "3589    I am in escape theatre now. . Going to watch K...\n",
      "3095    We walked from my moms. Right on stagwood pass...\n",
      "1012       I dunno they close oredi not... ÌÏ v ma fan...\n",
      "3320                               Yo im right by yo work\n",
      "4130    \\Its Ur luck to Love someone. Its Ur fortune t...\n",
      "1197     He also knows about lunch menu only da. . I know\n",
      "5426        Oh yeah! And my diet just flew out the window\n",
      "624     Nah it's straight, if you can just bring bud o...\n",
      "2260    SplashMobile: Choose from 1000s of gr8 tones e...\n",
      "Name: v2, dtype: object\n",
      "拆分过后每个邮件是否是垃圾邮件\n",
      "1114     ham\n",
      "3589     ham\n",
      "3095     ham\n",
      "1012     ham\n",
      "3320     ham\n",
      "4130     ham\n",
      "1197     ham\n",
      "5426     ham\n",
      "624      ham\n",
      "2260    spam\n",
      "Name: v1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 把数据拆分成为训练集和测试集\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(\n",
    "    df.v2,\n",
    "    df.v1, \n",
    "    test_size=0.2, \n",
    "    random_state=0)  \n",
    "\n",
    "print ('拆分过后的每个邮件内容')\n",
    "print (data_train[:10])\n",
    "print ('拆分过后每个邮件是否是垃圾邮件')\n",
    "print (labels_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4456    Aight should I just plan to come up later toni...\n",
      "690                                    Was the farm open?\n",
      "944     I sent my scores to sophas and i had to do sec...\n",
      "3768    Was gr8 to see that message. So when r u leavi...\n",
      "1189    In that case I guess I'll see you at campus lodge\n",
      "4437    Nothing will ever be easy. But don't be lookin...\n",
      "3587    If you were/are free i can give. Otherwise nal...\n",
      "1982    Hey i will be late... i'm at amk. Need to drin...\n",
      "2038          Hey are we going for the lo lesson or gym? \n",
      "2078                       85233 FREE>Ringtone!Reply REAL\n",
      "Name: v2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (data_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1115,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立词汇表，统计两个类目下面的共词计数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    用一个dictionary保存词汇，并给每个词汇赋予唯一的一个id\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    用一个dictionary保存词汇，并给每个词汇赋予唯一的一个id\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetVocabulary(data):\n",
    "    vocab_dict={}    #以单词为key，以id 为 value。 id 从o开始\n",
    "    wid=0\n",
    "    for document in data:\n",
    "        words=document.split() # 按空格分词“I am a student\" => ['I', 'am','a','student']\n",
    "        for word in words:\n",
    "            word = word.lower() #归一化，都小写\n",
    "            if word not in vocab_dict:\n",
    "                vocab_dict[word]=wid\n",
    "                wid += 1\n",
    "    return vocab_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all the unique words : 11706\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = GetVocabulary(data_train)\n",
    "print('Number of all the unique words : '+ str(len(vocab_dict.keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1114    No no:)this is kallis home ground.amla home to...\n",
       "3589    I am in escape theatre now. . Going to watch K...\n",
       "3095    We walked from my moms. Right on stagwood pass...\n",
       "1012       I dunno they close oredi not... ÌÏ v ma fan...\n",
       "3320                               Yo im right by yo work\n",
       "Name: v2, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all the unique words in test : 4957\n"
     ]
    }
   ],
   "source": [
    "# 以下的用不到，只是用来玩儿\n",
    "vocab_dict_test = GetVocabulary(data_test)\n",
    "print('Number of all the unique words in test : '+ str(len(vocab_dict_test.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把文字变成词向量  以便于计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "122\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def Document2Vector(vocab_dict,data):\n",
    "    word_vector = np.zeros(len(vocab_dict.keys()))\n",
    "    words = data.split()\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in vocab_dict:\n",
    "            word_vector[vocab_dict[word]]+=1\n",
    "    return word_vector\n",
    "\n",
    "#examples\n",
    "\n",
    "example = Document2Vector(vocab_dict,\" hello world\")\n",
    "print(example)\n",
    "print(vocab_dict['world'])\n",
    "print(example[vocab_dict['world']])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11706"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1., 1., 2., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 1., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 1., ..., 0., 0., 0.])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#把训练集的句子全部变成向量形式，这里面全是数字，每个词汇表里的单词 根据id排序的 出现在 该文章里的次数，即使没有出现 也是有 0\n",
    "train_matrix =[]\n",
    "for document in data_train.values:\n",
    "    word_vector = Document2Vector(vocab_dict,document)\n",
    "    train_matrix.append(word_vector)\n",
    "\n",
    "print(len(train_matrix))  # 有多少个文档\n",
    "train_matrix[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做naive bayes 训练 得到训练集每个词汇的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 2., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_matrix[0]))  #有多少个词汇\n",
    "train_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ee = np.ones(12)\n",
    "print(type(ee))\n",
    "ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on the doc id:0\n",
      "Train on the doc id:1000\n",
      "Train on the doc id:2000\n",
      "Train on the doc id:3000\n",
      "Train on the doc id:4000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    在训练集计算两种概率：\n",
    "        1. 词在每个分类下的概率，比如P('email'|Spam)\n",
    "        2. 每个分类的概率，比如P(Spam)\n",
    "        \n",
    "    这里的计算实现巧妙利用了numpy的array结构：\n",
    "        1. 在每个分类下创建一个与词汇量大小相等的vector(即 numpy array), 即spam_word_counter 和 ham_word_counter\n",
    "        2. 在遍历每一个句子的时候，直接与句子对应的vector相加，累积每个单词出现的次数\n",
    "        3. 在遍历完所有句子之后，再除以总词汇量，得到每个单词的概率\n",
    "'''\n",
    "\n",
    "def NaiveBayes_train(train_matrix,labels_train):\n",
    "    num_docs = len(train_matrix)\n",
    "    num_words = len(train_matrix[0])  # 对第一个样本取长度\n",
    "    \n",
    "    # 在每个分类下创建一个与词汇量大小相等的vector(即 numpy array) 用以计算每个单词在该类别下的频率\n",
    "    spam_word_counter =np.ones(num_words) \n",
    "    ham_word_counter = np.ones(num_words) #计算每个word出现的次数，初始化为1. 即使用拉普拉斯平滑\n",
    "    \n",
    "    spam_total_count =0 \n",
    "    ham_total_count = 0   #每一个类别 单词总的计数， 所有词出现在ham里头的总数 ham的总词数 （不去重 ）\n",
    "    \n",
    "    spam_count =0  #spam 邮件的总数\n",
    "    ham_count = 0\n",
    "    \n",
    "    for i in range(num_docs):\n",
    "        if i%1000==0:\n",
    "            print('Train on the doc id:'+ str(i))\n",
    "        \n",
    "        if labels_train[i] =='spam': #为什么这里是lables，因为前面已经从train_data里 提取了各种数据了，现在只要再跟label来对应就好\n",
    "            spam_word_counter += train_matrix[i]\n",
    "            spam_total_count += sum(train_matrix[i]) # 这里不能用spam_word_counter 因为会有重复，用matrix[i] 则不会重复\n",
    "            spam_count +=1\n",
    "        else:\n",
    "            ham_word_counter += train_matrix[i]\n",
    "            ham_total_count += sum(train_matrix[i])\n",
    "            ham_count += 1\n",
    "            \n",
    "    #spam_word_counter => 每个词的计数\n",
    "    #spam_total_count => Spam的总次数\n",
    "    #spam_count => Spam邮件计数\n",
    "    \n",
    "    #以下则是，每个单词 在各类别下出现的概率，并且取了log，为什么取log，就是怕太小变成0～这部分再看看了解下为什么\n",
    "    #并且注意 在分母上也要加上平滑部分\n",
    "    p_spam_vector = np.log(spam_word_counter/(spam_total_count+num_words))\n",
    "    p_ham_vector = np.log(ham_word_counter/(ham_total_count+num_words))\n",
    "\n",
    "    return p_spam_vector, np.log(spam_count/num_docs), p_ham_vector,np.log(ham_count/num_docs)\n",
    "\n",
    "p_spam_vector, p_spam, p_ham_vector, p_ham = NaiveBayes_train(train_matrix, labels_train.values)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11706"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -6.89649987, -10.1545964 ,  -5.30256614, ..., -10.1545964 ,\n",
       "       -10.1545964 , -10.1545964 ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_spam_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11706"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on the doc id: 0\n",
      "test on the doc id: 200\n",
      "test on the doc id: 400\n",
      "test on the doc id: 600\n",
      "test on the doc id: 800\n",
      "test on the doc id: 1000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    对测试集进行预测，按照公式计算例子在两个分类下的概率，选择概率较大者作为预测结果\n",
    "'''\n",
    "\n",
    "def Predict(test_word_vector, p_spam_vector,p_spam,p_ham_vector,p_ham):\n",
    "    \n",
    "    spam = sum(test_word_vector * p_spam_vector)+ p_spam\n",
    "    ham = sum(test_word_vector * p_ham_vector) + p_ham\n",
    "    \n",
    "    if spam > ham:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'ham'\n",
    "    \n",
    "predictions =[]\n",
    "\n",
    "i = 0\n",
    "for document in data_test.values:\n",
    "    if i %200 ==0:\n",
    "        print('test on the doc id: '+str(i))\n",
    "    i+=1\n",
    "    test_word_vector = Document2Vector(vocab_dict, document)\n",
    "    ans = Predict(test_word_vector, p_spam_vector, p_spam,p_ham_vector,p_ham)\n",
    "    predictions.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11706"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11706"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9775784753363229\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.98      1.00      0.99       949\n",
      "       spam       0.99      0.86      0.92       166\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1115\n",
      "\n",
      "[[948   1]\n",
      " [ 24 142]]\n"
     ]
    }
   ],
   "source": [
    "# 检测模型\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "print (accuracy_score(labels_test, predictions))\n",
    "print (classification_report(labels_test, predictions))\n",
    "print (confusion_matrix(labels_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
